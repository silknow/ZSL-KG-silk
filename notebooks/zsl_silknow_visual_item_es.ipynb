{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'es'\n",
    "assert(language in ['en', 'fr', 'es'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberbatch = pickle.load(open(f\"../neighborhoods/numberbatch-{language}-19.08.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(numberbatch.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls datasets/silknow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_techniques = pd.read_csv('../datasets/es_visual_item_post2.csv').rename(columns={'technique':'label'})\n",
    "ds_techniques.label = ds_techniques.label.str.lower()\n",
    "ds_techniques.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_techniques.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds_techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_neighborhood(word, depth=2, allowed_rels='all', keep='top20000', language=language):\n",
    "    neighborhood = pickle.load(open('/data/zeste_cache/neighborhoods_'+language+'/'+word+'.pickle', 'rb'))\n",
    "    neighborhood_words = list(neighborhood.keys())\n",
    "    \n",
    "    if allowed_rels != 'all':\n",
    "        for n in neighborhood_words:\n",
    "            if all(rel not in neighborhood[n]['rels'] for rel in allowed_rels):\n",
    "                del neighborhood[n]\n",
    "                continue\n",
    "\n",
    "    to_visit_next = list(neighborhood.keys())\n",
    "    \n",
    "    while depth > 1:\n",
    "        additions = []\n",
    "        while len(to_visit_next) > 0:\n",
    "            w = to_visit_next.pop()\n",
    "            nn = get_word_neighborhood(w, depth=1, allowed_rels=allowed_rels)\n",
    "            for ww in nn:\n",
    "                if ww in neighborhood:\n",
    "                    neighborhood[ww]['from'].append(w)\n",
    "                    neighborhood[ww]['rels'].extend(['<>'] + nn[ww]['rels'])\n",
    "                else:\n",
    "                    neighborhood[ww] = {}\n",
    "                    neighborhood[ww]['from'] = [w]\n",
    "                    neighborhood[ww]['rels'] = nn[ww]['rels']\n",
    "                    if '/c/'+language+'/'+word in numberbatch and '/c/'+language+'/'+ww in numberbatch:\n",
    "                        neighborhood[ww]['sim'] = numberbatch.similarity('/c/'+language+'/'+word, '/c/'+language+'/'+ww)\n",
    "                    else:\n",
    "                        neighborhood[ww]['sim'] = 0.0\n",
    "                    additions.append(ww)\n",
    "        to_visit_next = additions\n",
    "        depth -= 1\n",
    "    \n",
    "    if keep.startswith('top'):\n",
    "        k = int(keep.split('top')[1])\n",
    "        all_scores = [neighborhood[kw]['sim'] for kw in neighborhood]\n",
    "        all_words = list(neighborhood.keys())\n",
    "        if k < len(all_scores):\n",
    "            lowest_top = sorted(all_scores, reverse=True)[k]\n",
    "            for kw in all_words:\n",
    "                if neighborhood[kw]['sim'] <= lowest_top:\n",
    "                    del neighborhood[kw]\n",
    "    \n",
    "    return neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    doc = ''.join(c for c in doc if c not in string.punctuation or c == ' ').lower()\n",
    "    doc_tokens = doc.split(' ')\n",
    "    doc_tokens = [w.lower() for w in doc_tokens]\n",
    "    doc_tokens = [w for w in doc_tokens if w not in stopwords.words('english')]\n",
    "    doc_tokens = [w.replace('\"', '').replace('?', '') for w in doc_tokens]\n",
    "    doc_tokens = [w.replace('-', '_') for w in doc_tokens if w != '']\n",
    "\n",
    "    return doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_neighborhood(words, depth=2, allowed_rels='all', keep='top20000', language=language):\n",
    "    words = words.split(';')\n",
    "    ns = []\n",
    "    \n",
    "    for word in words:\n",
    "        ns.append(get_word_neighborhood(word, depth=depth, allowed_rels=allowed_rels, keep=keep, language=language))\n",
    "    neighborhood = ns[0].copy()\n",
    "    \n",
    "    for w, nn in zip(words[1:], ns[1:]):\n",
    "        for ww in nn:\n",
    "            if ww in neighborhood:\n",
    "                neighborhood[ww]['from'].append(w)\n",
    "                neighborhood[ww]['rels'].extend(['<>'] + nn[ww]['rels'])\n",
    "                neighborhood[ww]['sim'] = max(neighborhood[ww]['sim'], nn[ww]['sim'])\n",
    "            else:\n",
    "                neighborhood[ww] = {}\n",
    "                neighborhood[ww]['from'] = [w]\n",
    "                neighborhood[ww]['rels'] = nn[ww]['rels']\n",
    "                neighborhood[ww]['sim']  = nn[ww]['sim']\n",
    "\n",
    "    return neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(ds_techniques.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_neighborhood('rose', 1, 'all').keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mapping = {  'brocade': 'brocade',\n",
    "             'brocatelle': 'brocatelle',\n",
    "             'chiné': 'chiné',\n",
    "             'damask': 'damask',\n",
    "             'embroidery': \"embroidery\",\n",
    "             'florentine': \"florentine\",\n",
    "             'gros': \"gros\",\n",
    "             'jacquard weave': \"jacquard_loom\",\n",
    "             'lampas': \"lampas\",\n",
    "             'moiré' : 'moiré',\n",
    "             'pattern weft': \"weft\",\n",
    "             'plain': \"plain\",\n",
    "             'velvet': \"velvet\"}\"\"\"\n",
    "\n",
    "\n",
    "mapping = { 'vegetal motif':'planta',\n",
    "            'floral motif':'flor',\n",
    "            'geometrical motif':'geometría',\n",
    "            'fleur-de-lis':'flor',\n",
    "            'bunch':'flor',\n",
    "            'vine':'planta',\n",
    "            'thistle':'planta',\n",
    "            'rhombus':'geometría',\n",
    "            'rose':'flor',\n",
    "            'leaf':'planta'           \n",
    "            }\n",
    "\n",
    "labels = sorted(set(mapping.values()))\n",
    "print(len(labels))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pbar = tqdm(labels)\n",
    "\n",
    "labels_cgr = {}\n",
    "for label in pbar:\n",
    "    pbar.set_description(label)\n",
    "    labels_cgr[label] = get_words_neighborhood(label, 2, 'all', keep='top20000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{l:len(labels_cgr[l]) for l in labels_cgr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cgr.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_score(doc, label_neighborhood):\n",
    "    # tokens = preprocess(doc)\n",
    "    if type(doc) == str:\n",
    "        doc = doc.split(' ')\n",
    "    tokens = doc\n",
    "    related_words = []\n",
    "    score = 0\n",
    "    for token in tokens: \n",
    "        if token in label_neighborhood:\n",
    "            similarity = label_neighborhood[token]['sim']\n",
    "            if similarity > 0:\n",
    "                related_words.append((token, label_neighborhood[token]['rels'][0], similarity))\n",
    "                score += similarity\n",
    "        \n",
    "    return score# , sorted(related_words, key=lambda t: -t[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_techniques.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_silknow = ds_techniques.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    data_preprocessed = pool.map(preprocess, ds_techniques.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_silknow(doc, labels_ns=labels_cgr):\n",
    "    return np.argmax([get_document_score(doc, labels_ns[l]) for l in sorted(labels_ns.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(corpus_preprocessed, labels_cgr):\n",
    "    with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "        predictions = pool.map(scoring_silknow, corpus_preprocessed)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = generate_predictions(data_preprocessed, labels_cgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction_labels = [labels[p] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [mapping[l] for l in ds_techniques.label.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(prediction_labels, true_labels, digits=3))\n",
    "m = np.asarray(prediction_labels)\n",
    "ds_techniques[\"predictions\"] = m \n",
    "ds_techniques.to_csv('visual-items-es-predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(prediction_labels, true_labels)\n",
    "cm_df = pd.DataFrame(cm, columns=labels, index=labels)\n",
    "plt.figure(figsize = (5,5))\n",
    "sns.heatmap(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Top-k\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "def scoring_silknow_top_n(doc, labels_ns=labels_cgr, n=k):\n",
    "    return np.argsort([get_document_score(doc, labels_ns[l]) for l in sorted(labels_ns.keys())])[:-(n+1):-1]\n",
    "def generate_top_predictions(corpus_preprocessed, labels_cgr):\n",
    "    with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "        predictions = pool.map(scoring_silknow_top_n, corpus_preprocessed)\n",
    "    return predictions\n",
    "true_labels = [mapping[l] for l in ds_techniques.label.tolist()]\n",
    "topk_predictions_lists = generate_top_predictions(data_preprocessed, labels_cgr)\n",
    "prediction_labels_lists = [[labels[l] for l in p] for p in topk_predictions_lists]\n",
    "topk_predictions = []\n",
    "\n",
    "for i, gt_label in enumerate(true_labels):\n",
    "    topk_predictions.append(gt_label if gt_label in prediction_labels_lists[i] else prediction_labels_lists[i][0])\n",
    "print(classification_report(topk_predictions, true_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "def scoring_silknow_top_n(doc, labels_ns=labels_cgr, n=k):\n",
    "    return np.argsort([get_document_score(doc, labels_ns[l]) for l in sorted(labels_ns.keys())])[:-(n+1):-1]\n",
    "def generate_top_predictions(corpus_preprocessed, labels_cgr):\n",
    "    with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "        predictions = pool.map(scoring_silknow_top_n, corpus_preprocessed)\n",
    "    return predictions\n",
    "true_labels = [mapping[l] for l in ds_techniques.label.tolist()]\n",
    "topk_predictions_lists = generate_top_predictions(data_preprocessed, labels_cgr)\n",
    "prediction_labels_lists = [[labels[l] for l in p] for p in topk_predictions_lists]\n",
    "topk_predictions = []\n",
    "\n",
    "for i, gt_label in enumerate(true_labels):\n",
    "    topk_predictions.append(gt_label if gt_label in prediction_labels_lists[i] else prediction_labels_lists[i][0])\n",
    "print(classification_report(topk_predictions, true_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
